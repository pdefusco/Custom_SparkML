{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "359959d0-1482-4c75-bab1-eab5a7565ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, os\n",
    "import numpy as np\n",
    "from pyspark.sql import Row\n",
    "from sklearn import neighbors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.mllib.stat import Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "005fe32f-1be4-4e3a-b8e5-e0b118ebb087",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, StandardScaler, Imputer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.mllib.stat import Statistics\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3f772ca-03f3-469f-b952-a534dbc05bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07b0b509-7c25-49b5-afee-3d26b5029761",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1c69174-fb47-4684-9910-19606e7eaace",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting spark.hadoop.yarn.resourcemanager.principal to pauldefusco\n",
      "Hive Session ID = a7d8ef99-0510-491e-b82e-c41d5d612114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           namespace|\n",
      "+--------------------+\n",
      "|         01_car_data|\n",
      "|           01_car_dw|\n",
      "|             airline|\n",
      "|          airline_dw|\n",
      "|            airlines|\n",
      "|        airlines_csv|\n",
      "|       airlines_csv1|\n",
      "|   airlines_csv_vish|\n",
      "|    airlines_iceberg|\n",
      "|   airlines_iceberg1|\n",
      "|airlines_iceberg_...|\n",
      "|          airquality|\n",
      "|          atlas_demo|\n",
      "|            bankdemo|\n",
      "|              bhagan|\n",
      "|             cdedemo|\n",
      "|        cdp_overview|\n",
      "|        cgsifacebook|\n",
      "|               claim|\n",
      "|           clev_bank|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cml.data_v1 as cmldata\n",
    "\n",
    "CONNECTION_NAME = \"go01-aw-dl\"\n",
    "conn = cmldata.get_connection(CONNECTION_NAME)\n",
    "spark = conn.get_spark_session()\n",
    "\n",
    "# Sample usage to run query through spark\n",
    "EXAMPLE_SQL_QUERY = \"show databases\"\n",
    "spark.sql(EXAMPLE_SQL_QUERY).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eab56e0-ea18-4ee4-8620-33ad2f72ba00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTransformer(Transformer, HasInputCol, HasOutputCol, DefaultParamsReadable, DefaultParamsWritable):\n",
    "    input_col = Param(Params._dummy(), \"input_col\", \"input column name.\", typeConverter=TypeConverters.toString)\n",
    "    output_col = Param(Params._dummy(), \"output_col\", \"output column name.\", typeConverter=TypeConverters.toString)\n",
    "\n",
    "    @keyword_only\n",
    "    def __init__(self, input_col: str = \"input\", output_col: str = \"output\"):\n",
    "        super(CustomTransformer, self).__init__()\n",
    "        self._setDefault(input_col=None, output_col=None)\n",
    "        kwargs = self._input_kwargs\n",
    "        self.set_params(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def set_params(self, input_col: str = \"input\", output_col: str = \"output\"):\n",
    "        kwargs = self._input_kwargs\n",
    "        self._set(**kwargs)\n",
    "\n",
    "    def get_input_col(self):\n",
    "        return self.getOrDefault(self.input_col)\n",
    "\n",
    "    def get_output_col(self):\n",
    "        return self.getOrDefault(self.output_col)\n",
    "\n",
    "    def _transform(self, df: DataFrame):\n",
    "        input_col = self.get_input_col()\n",
    "        output_col = self.get_output_col()\n",
    "        # The custom action: concatenate the integer form of the doubles from the Vector\n",
    "        transform_udf = F.udf(lambda x: '/'.join([str(int(y)) for y in x]), StringType())\n",
    "        return df.withColumn(output_col, transform_udf(input_col))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
